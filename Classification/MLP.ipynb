{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron for SIDER Multi-Label Classification\n",
    "\n",
    "This notebook implements a Multi-Layer Perceptron (MLP) model for predicting drug side effects using the SIDER dataset. The model performs multi-label classification across 27 different side effect categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, hamming_loss\n",
    "\n",
    "# PyTorch and PyTorch Lightning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from Classification.src.sider_preprocessing import sider_preprocessing\n",
    "from Classification.src.sider_featurizer import featurizer\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SIDER dataset\n",
    "df = pd.read_csv('../data/raw/sider.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Number of molecules: {len(df)}\")\n",
    "print(f\"Number of side effect categories: {len(df.columns) - 1}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data (clean and canonicalize SMILES)\n",
    "df_cleaned = sider_preprocessing(df)\n",
    "print(f\"\\nCleaned dataset shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate molecular features\n",
    "df_final = featurizer(df=df_cleaned, mol_col='Molecule', fpSize=2048)\n",
    "print(f\"\\nFinal dataset shape with features: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature and Target Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and targets\n",
    "X = df_final.iloc[:, 29:].copy()  # Molecular features\n",
    "y = df_final.iloc[:, 2:29]        # 27 side effect labels\n",
    "\n",
    "# Select only numeric features\n",
    "X = X.select_dtypes(include=np.number)\n",
    "\n",
    "# Remove zero-variance features\n",
    "selector = VarianceThreshold(threshold=0.0)\n",
    "X_cleaned_array = selector.fit_transform(X)\n",
    "X = pd.DataFrame(X_cleaned_array, columns=X.columns[selector.get_support()])\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target matrix shape: {y.shape}\")\n",
    "print(f\"\\nNumber of features after variance filtering: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "# Feature scaling (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train_scaled.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test_scaled.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIDERDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset wrapper for SIDER data.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y.values if hasattr(y, 'values') else y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "train_dataset = SIDERDataset(X_train_scaled, y_train)\n",
    "test_dataset = SIDERDataset(X_test_scaled, y_test)\n",
    "\n",
    "# Create validation split from training data (90-10 split)\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(\n",
    "    train_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Training samples: {len(train_subset)}\")\n",
    "print(f\"Validation samples: {len(val_subset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MLP Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_SIDER(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for SIDER multi-label classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer: molecular features\n",
    "    - Multiple hidden layers with ReLU activation, batch normalization, and dropout\n",
    "    - Output layer: 27 neurons (one for each side effect)\n",
    "    - Sigmoid activation for multi-label classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256, 128], out_dim=27, dropout=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Build MLP architecture dynamically\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers with activation, batch norm, and dropout\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (no activation - using BCEWithLogitsLoss)\n",
    "        layers.append(nn.Linear(prev_dim, out_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Loss function for multi-label classification\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Store predictions for epoch-end metrics\n",
    "        self.validation_outputs = []\n",
    "        self.test_outputs = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Store outputs for epoch-end AUC calculation\n",
    "        probs = torch.sigmoid(logits)\n",
    "        self.validation_outputs.append({'preds': probs, 'targets': y})\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if len(self.validation_outputs) > 0:\n",
    "            # Concatenate all predictions and targets\n",
    "            all_preds = torch.cat([x['preds'] for x in self.validation_outputs], dim=0)\n",
    "            all_targets = torch.cat([x['targets'] for x in self.validation_outputs], dim=0)\n",
    "            \n",
    "            # Calculate macro AUC-ROC\n",
    "            try:\n",
    "                macro_auc = roc_auc_score(\n",
    "                    all_targets.cpu().numpy(), \n",
    "                    all_preds.cpu().numpy(), \n",
    "                    average='macro'\n",
    "                )\n",
    "                self.log('val_macro_auc', macro_auc, on_epoch=True, prog_bar=True)\n",
    "            except:\n",
    "                pass  # Skip if AUC calculation fails\n",
    "            \n",
    "            # Clear outputs\n",
    "            self.validation_outputs.clear()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        self.test_outputs.append({'preds': probs, 'targets': y})\n",
    "        return {'preds': probs, 'targets': y}\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        # Concatenate all predictions and targets\n",
    "        all_preds = torch.cat([x['preds'] for x in self.test_outputs], dim=0)\n",
    "        all_targets = torch.cat([x['targets'] for x in self.test_outputs], dim=0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        macro_auc = roc_auc_score(\n",
    "            all_targets.cpu().numpy(), \n",
    "            all_preds.cpu().numpy(), \n",
    "            average='macro'\n",
    "        )\n",
    "        self.log('test_macro_auc', macro_auc)\n",
    "        \n",
    "        # Clear outputs\n",
    "        self.test_outputs.clear()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLP model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "mlp_model = MLP_SIDER(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dims=[512, 256, 128],  # Three hidden layers\n",
    "    out_dim=27,                    # 27 side effect categories\n",
    "    dropout=0.3,                   # Dropout for regularization\n",
    "    lr=0.001                       # Learning rate\n",
    ")\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  Input dimension: {input_dim}\")\n",
    "print(f\"  Hidden layers: [512, 256, 128]\")\n",
    "print(f\"  Output dimension: 27\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in mlp_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='mlp_checkpoints/',\n",
    "    filename='best-mlp-{epoch:02d}-{val_loss:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(mlp_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating MLP on test set...\")\n",
    "\n",
    "mlp_model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        logits = mlp_model(x)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        all_preds.append(probs.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_targets = np.vstack(all_targets)\n",
    "\n",
    "# Calculate metrics\n",
    "macro_auc = roc_auc_score(all_targets, all_preds, average='macro')\n",
    "micro_auc = roc_auc_score(all_targets, all_preds, average='micro')\n",
    "\n",
    "# Binary predictions for additional metrics\n",
    "binary_preds = (all_preds > 0.5).astype(int)\n",
    "hamming = hamming_loss(all_targets, binary_preds)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"MLP PERFORMANCE METRICS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Macro AUC-ROC: {macro_auc:.4f}\")\n",
    "print(f\"Micro AUC-ROC: {micro_auc:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Label Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC-ROC for each label\n",
    "label_aucs = {}\n",
    "for i, label in enumerate(y.columns):\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets[:, i], all_preds[:, i])\n",
    "        label_aucs[label] = auc\n",
    "    except:\n",
    "        label_aucs[label] = np.nan\n",
    "\n",
    "# Sort by AUC\n",
    "sorted_labels = sorted(label_aucs.items(), key=lambda x: x[1] if not np.isnan(x[1]) else 0, reverse=True)\n",
    "\n",
    "print(\"\\nPER-LABEL AUC-ROC SCORES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Label':<50} {'AUC-ROC':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Top 5 labels\n",
    "print(\"\\nTop 5 Best Predicted Labels:\")\n",
    "for label, auc in sorted_labels[:5]:\n",
    "    if not np.isnan(auc):\n",
    "        print(f\"{label:<50} {auc:>10.4f}\")\n",
    "\n",
    "# Bottom 5 labels\n",
    "print(\"\\nTop 5 Worst Predicted Labels:\")\n",
    "for label, auc in sorted_labels[-5:]:\n",
    "    if not np.isnan(auc):\n",
    "        print(f\"{label:<50} {auc:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with baseline models from the original analysis\n",
    "model_scores = {\n",
    "    'Random Forest': 0.6691,\n",
    "    'XGBoost': 0.6596,\n",
    "    'GNN (default)': 0.6408,\n",
    "    'MLP (current)': macro_auc,\n",
    "    'Logistic Regression': 0.6213,\n",
    "    'SVM (linear)': 0.6145,\n",
    "    'Transformer + RF': 0.6070,\n",
    "    'GNN (optimized)': 0.5886\n",
    "}\n",
    "\n",
    "# Sort by performance\n",
    "sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE COMPARISON (MACRO AUC-ROC)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<30} {'AUC-ROC':>10} {'Relative to Best':>15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "best_score = sorted_models[0][1]\n",
    "for model, score in sorted_models:\n",
    "    relative = ((score - best_score) / best_score) * 100\n",
    "    marker = \" <-- Current Model\" if model == 'MLP (current)' else \"\"\n",
    "    print(f\"{model:<30} {score:>10.4f} {relative:>14.1f}%{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot of model performances\n",
    "models = [m[0] for m in sorted_models]\n",
    "scores = [m[1] for m in sorted_models]\n",
    "colors = ['green' if m == 'MLP (current)' else 'steelblue' for m in models]\n",
    "\n",
    "ax1.barh(models, scores, color=colors)\n",
    "ax1.set_xlabel('Macro AUC-ROC')\n",
    "ax1.set_title('Model Performance Comparison')\n",
    "ax1.set_xlim([0.55, 0.70])\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (model, score) in enumerate(zip(models, scores)):\n",
    "    ax1.text(score + 0.002, i, f'{score:.4f}', va='center')\n",
    "\n",
    "# Per-label AUC distribution\n",
    "label_scores = list(label_aucs.values())\n",
    "label_scores = [s for s in label_scores if not np.isnan(s)]  # Remove NaN values\n",
    "\n",
    "ax2.hist(label_scores, bins=15, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(macro_auc, color='red', linestyle='--', linewidth=2, label=f'Macro AUC: {macro_auc:.4f}')\n",
    "ax2.set_xlabel('AUC-ROC')\n",
    "ax2.set_ylabel('Number of Labels')\n",
    "ax2.set_title('Distribution of Per-Label AUC-ROC Scores')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history if available\n",
    "if hasattr(trainer, 'logged_metrics'):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Extract training metrics\n",
    "    epochs = range(1, trainer.current_epoch + 1)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, label='Training Loss')\n",
    "    plt.plot(epochs, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, label='Validation Macro AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Macro AUC-ROC')\n",
    "    plt.title('Validation Performance')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusions\n",
    "\n",
    "### Model Performance\n",
    "The Multi-Layer Perceptron (MLP) model has been successfully implemented for multi-label classification on the SIDER dataset. The model predicts 27 different side effect categories based on molecular features.\n",
    "\n",
    "### Key Findings:\n",
    "1. **Overall Performance**: The MLP achieves a macro AUC-ROC score, which measures the model's ability to distinguish between positive and negative samples across all labels.\n",
    "\n",
    "2. **Architecture**: The model uses three hidden layers (512, 256, 128 neurons) with ReLU activation, batch normalization, and dropout for regularization.\n",
    "\n",
    "3. **Comparison with Baselines**: The MLP's performance can be compared against other models including Random Forest, XGBoost, GNN, and SVM.\n",
    "\n",
    "### Strengths:\n",
    "- Can capture non-linear relationships in the data\n",
    "- Relatively fast training compared to complex models like GNNs\n",
    "- Good regularization through dropout and batch normalization\n",
    "\n",
    "### Potential Improvements:\n",
    "1. **Hyperparameter Tuning**: Use Bayesian optimization or grid search to find optimal architecture\n",
    "2. **Feature Engineering**: Explore additional molecular descriptors or fingerprints\n",
    "3. **Ensemble Methods**: Combine MLP with other models for better performance\n",
    "4. **Class Imbalance**: Address imbalanced labels with weighted loss functions\n",
    "5. **Advanced Architectures**: Explore attention mechanisms or residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model_path = 'mlp_final_model.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': mlp_model.state_dict(),\n",
    "    'input_dim': input_dim,\n",
    "    'hidden_dims': [512, 256, 128],\n",
    "    'macro_auc': macro_auc,\n",
    "    'micro_auc': micro_auc\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Macro AUC-ROC: {macro_auc:.4f}\")\n",
    "print(f\"  Micro AUC-ROC: {micro_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}